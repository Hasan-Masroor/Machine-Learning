{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Problem 3: Feature Selection and Classification Using Weighted K-Nearest Neighbors (KNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Data Preparation:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('P3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.140023</td>\n",
       "      <td>0.265418</td>\n",
       "      <td>-0.535880</td>\n",
       "      <td>0.808262</td>\n",
       "      <td>-0.998551</td>\n",
       "      <td>1.147053</td>\n",
       "      <td>0.979441</td>\n",
       "      <td>0.722351</td>\n",
       "      <td>0.559936</td>\n",
       "      <td>0.399382</td>\n",
       "      <td>1.098400</td>\n",
       "      <td>0.036805</td>\n",
       "      <td>1.302542</td>\n",
       "      <td>-0.239813</td>\n",
       "      <td>0.625914</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.176099</td>\n",
       "      <td>1.690023</td>\n",
       "      <td>-0.737042</td>\n",
       "      <td>1.207350</td>\n",
       "      <td>-1.249740</td>\n",
       "      <td>-0.921881</td>\n",
       "      <td>0.065195</td>\n",
       "      <td>-0.581772</td>\n",
       "      <td>0.645254</td>\n",
       "      <td>-0.089174</td>\n",
       "      <td>-1.571598</td>\n",
       "      <td>0.633757</td>\n",
       "      <td>0.636572</td>\n",
       "      <td>1.233946</td>\n",
       "      <td>-0.300362</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.322751</td>\n",
       "      <td>0.635943</td>\n",
       "      <td>0.012525</td>\n",
       "      <td>0.027479</td>\n",
       "      <td>0.083267</td>\n",
       "      <td>0.447982</td>\n",
       "      <td>1.158902</td>\n",
       "      <td>1.177922</td>\n",
       "      <td>0.887521</td>\n",
       "      <td>0.018378</td>\n",
       "      <td>-0.395321</td>\n",
       "      <td>0.451929</td>\n",
       "      <td>0.744254</td>\n",
       "      <td>-0.492224</td>\n",
       "      <td>-0.316476</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.907289</td>\n",
       "      <td>-0.475599</td>\n",
       "      <td>-1.270970</td>\n",
       "      <td>1.837189</td>\n",
       "      <td>-2.471438</td>\n",
       "      <td>1.270536</td>\n",
       "      <td>-0.536260</td>\n",
       "      <td>-0.852590</td>\n",
       "      <td>0.778421</td>\n",
       "      <td>0.362119</td>\n",
       "      <td>-1.111151</td>\n",
       "      <td>0.153531</td>\n",
       "      <td>-0.342588</td>\n",
       "      <td>-0.147104</td>\n",
       "      <td>-0.585484</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.809604</td>\n",
       "      <td>1.039083</td>\n",
       "      <td>0.869025</td>\n",
       "      <td>-1.204626</td>\n",
       "      <td>1.756469</td>\n",
       "      <td>0.318814</td>\n",
       "      <td>-2.582921</td>\n",
       "      <td>-1.493409</td>\n",
       "      <td>-1.221354</td>\n",
       "      <td>0.435062</td>\n",
       "      <td>-0.068407</td>\n",
       "      <td>0.942154</td>\n",
       "      <td>0.329038</td>\n",
       "      <td>0.375319</td>\n",
       "      <td>-0.843292</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        f1        f2        f3        f4        f5        f6  \\\n",
       "0           0  0.140023  0.265418 -0.535880  0.808262 -0.998551  1.147053   \n",
       "1           1  1.176099  1.690023 -0.737042  1.207350 -1.249740 -0.921881   \n",
       "2           2  1.322751  0.635943  0.012525  0.027479  0.083267  0.447982   \n",
       "3           3  0.907289 -0.475599 -1.270970  1.837189 -2.471438  1.270536   \n",
       "4           4  0.809604  1.039083  0.869025 -1.204626  1.756469  0.318814   \n",
       "\n",
       "         f7        f8        f9       f10       f11       f12       f13  \\\n",
       "0  0.979441  0.722351  0.559936  0.399382  1.098400  0.036805  1.302542   \n",
       "1  0.065195 -0.581772  0.645254 -0.089174 -1.571598  0.633757  0.636572   \n",
       "2  1.158902  1.177922  0.887521  0.018378 -0.395321  0.451929  0.744254   \n",
       "3 -0.536260 -0.852590  0.778421  0.362119 -1.111151  0.153531 -0.342588   \n",
       "4 -2.582921 -1.493409 -1.221354  0.435062 -0.068407  0.942154  0.329038   \n",
       "\n",
       "        f14       f15  label  \n",
       "0 -0.239813  0.625914    0.0  \n",
       "1  1.233946 -0.300362    1.0  \n",
       "2 -0.492224 -0.316476    1.0  \n",
       "3 -0.147104 -0.585484    0.0  \n",
       "4  0.375319 -0.843292    1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)  # or .tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of numerical features\n",
    "features = df.iloc[:,:-1]\n",
    "target_value = df.iloc[:,-1]\n",
    "\n",
    "def minmax_norm(X:pd.Series):\n",
    "    return (X - np.min(X)) / (np.max(X) - np.min(X))\n",
    "\n",
    "for col in features.columns:\n",
    "    df[col] = minmax_norm(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stratified_train_test_split(df, target_column, test_size=0.3):\n",
    "    train_set = pd.DataFrame()\n",
    "    test_set = pd.DataFrame()\n",
    "\n",
    "    classes = df[target_column].unique()\n",
    "\n",
    "    for class_ in classes:\n",
    "        class_subset = df[df[target_column] == class_]\n",
    "        \n",
    "        test_subset_size = int(len(class_subset) * test_size)\n",
    "        \n",
    "        test_subset = class_subset.sample(test_subset_size)\n",
    "        train_subset = class_subset.drop(test_subset.index)\n",
    "\n",
    "        train_set = pd.concat([train_set, train_subset])\n",
    "        test_set = pd.concat([test_set, test_subset])\n",
    "\n",
    "    return train_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df , test_df = stratified_train_test_split(df,'label',0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 17)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 17)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Weighted KNN Loss Calculation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_knn(weights, train_points:pd.DataFrame, new_points:pd.DataFrame ,k = 5 ,target_value_name = 'label'):\n",
    "    predictions = []\n",
    "    for _, point in new_points.iterrows():\n",
    "        distances = []\n",
    "        for _, p in train_points.iterrows():\n",
    "            counter = 0\n",
    "            distance = 0\n",
    "            for col in train_points.columns:\n",
    "                if col != 'label' and col != 'Unnamed: 0':\n",
    "                    distance += (weights[counter] * ((point[col] - p[col])**2))\n",
    "                    counter += 1\n",
    "            distance = np.sqrt(distance)\n",
    "            distance = round(distance,ndigits=2)\n",
    "            distances.append((distance,p[target_value_name]))\n",
    "        # print(distances)\n",
    "        distances = sorted(distances)[:k]\n",
    "\n",
    "        counter_class_0 = 0\n",
    "        counter_class_1 = 0\n",
    "\n",
    "        for d in distances:\n",
    "            if d[1] == 0:\n",
    "                counter_class_0 += 1\n",
    "            else:\n",
    "                counter_class_1 += 1\n",
    "\n",
    "        if counter_class_0 > counter_class_1:\n",
    "            predictions.append(0)\n",
    "        else:\n",
    "            predictions.append(1)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [1 for i in range(15)]\n",
    "pred = weighted_knn(weights ,train_df,test_df.iloc[:,:-1],k=5,target_value_name='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(Y_true,Y_pred):\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i in range(len(Y_pred)):\n",
    "        if Y_pred[i] == Y_true[i]:\n",
    "            if Y_true[i] == 0:\n",
    "                TN += 1\n",
    "            elif Y_true[i] == 1:\n",
    "                TP += 1\n",
    "        if Y_pred[i] != Y_true[i]:\n",
    "            if Y_true[i] == 0:\n",
    "                FP += 1\n",
    "            elif Y_true[i] == 1:\n",
    "                FN += 1\n",
    "    return TP,TN,FP,FN\n",
    "\n",
    "def precision(Y_true,Y_pred):\n",
    "    TP,TN,FP,FN = confusion_matrix(Y_true,Y_pred)\n",
    "    if TP+FP == 0:\n",
    "        return np.inf\n",
    "    return (TP)/(TP+FP)\n",
    "\n",
    "def recall(Y_true,Y_pred):\n",
    "    TP,TN,FP,FN = confusion_matrix(Y_true,Y_pred)\n",
    "    return (TP) / (TP + FN)\n",
    "\n",
    "def f1_score(Y_true,Y_pred):\n",
    "    TP,TN,FP,FN = confusion_matrix(Y_true,Y_pred)\n",
    "    precision_ = precision(Y_true,Y_pred)\n",
    "    recall_ = recall(Y_true,Y_pred)\n",
    "    return (2*precision_*recall_)/(precision_ + recall_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score for weighted knn: 0.8148148148148148\n"
     ]
    }
   ],
   "source": [
    "print(f'f1 score for weighted knn: {f1_score(np.array(test_df.iloc[:,-1]),np.array(pred))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss for weighted knn: 5.756489385732788\n"
     ]
    }
   ],
   "source": [
    "print(f'log loss for weighted knn: {log_loss(np.array(test_df.iloc[:,-1]),np.array(pred))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_for_weighted_knn(weights ,train_df,test_df,k=5,target_value_name='label'):\n",
    "    pred = weighted_knn(weights ,train_df,test_df.iloc[:,:-1],k=5,target_value_name='label')\n",
    "    print(f'log loss for weighted knn: {log_loss(np.array(test_df.iloc[:,-1]),np.array(pred))}')\n",
    "    return log_loss(np.array(test_df.iloc[:,-1]),np.array(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss for weighted knn: 5.756489385732788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(5.756489385732788)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss_for_weighted_knn(weights ,train_df,test_df,k=5,target_value_name='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Weight Optimization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss for weighted knn: 17.269388197455335\n",
      "log loss for weighted knn: 17.269388197455335\n",
      "log loss for weighted knn: 12.664218011467248\n",
      "log loss for weighted knn: 17.269388197455335\n",
      "log loss for weighted knn: 17.269388197455335\n",
      "log loss for weighted knn: 17.269388197455335\n",
      "log loss for weighted knn: 17.269388197455335\n",
      "log loss for weighted knn: 17.269388197455335\n",
      "log loss for weighted knn: 14.966803104461292\n",
      "log loss for weighted knn: 17.269388197455335\n",
      "log loss for weighted knn: 17.269388197455335\n",
      "log loss for weighted knn: 17.269388197455335\n",
      "log loss for weighted knn: 17.269388197455335\n",
      "log loss for weighted knn: 17.269388197455335\n",
      "log loss for weighted knn: 17.269388197455335\n",
      "log loss for weighted knn: 17.269388197455335\n",
      "log loss for weighted knn: 8.059127785222179\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 8.059127785222179\n",
      "log loss for weighted knn: 8.059127785222179\n",
      "log loss for weighted knn: 8.059127785222179\n",
      "log loss for weighted knn: 8.059127785222179\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 8.059127785222179\n",
      "log loss for weighted knn: 8.059127785222179\n",
      "log loss for weighted knn: 8.059127785222179\n",
      "log loss for weighted knn: 8.059127785222179\n",
      "log loss for weighted knn: 8.059127785222179\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.90778193222981\n",
      "log loss for weighted knn: 6.90778193222981\n",
      "log loss for weighted knn: 6.90778193222981\n",
      "log loss for weighted knn: 6.90778193222981\n",
      "log loss for weighted knn: 6.90778193222981\n",
      "log loss for weighted knn: 6.90778193222981\n",
      "log loss for weighted knn: 6.90778193222981\n",
      "log loss for weighted knn: 6.90778193222981\n",
      "log loss for weighted knn: 6.90778193222981\n",
      "log loss for weighted knn: 6.90778193222981\n",
      "log loss for weighted knn: 6.90778193222981\n",
      "log loss for weighted knn: 6.90778193222981\n",
      "log loss for weighted knn: 6.90778193222981\n",
      "log loss for weighted knn: 6.90778193222981\n",
      "log loss for weighted knn: 6.90778193222981\n",
      "log loss for weighted knn: 6.90778193222981\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732787\n",
      "log loss for weighted knn: 5.756489385732787\n",
      "log loss for weighted knn: 5.756489385732787\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732787\n",
      "log loss for weighted knn: 5.756489385732787\n",
      "log loss for weighted knn: 5.756489385732787\n",
      "log loss for weighted knn: 5.756489385732787\n",
      "log loss for weighted knn: 5.756489385732787\n",
      "log loss for weighted knn: 5.756489385732787\n",
      "log loss for weighted knn: 5.756489385732787\n",
      "log loss for weighted knn: 5.756489385732787\n",
      "log loss for weighted knn: 5.756489385732787\n",
      "log loss for weighted knn: 5.756489385732787\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 4.605170185988092\n",
      "log loss for weighted knn: 4.605170185988092\n",
      "log loss for weighted knn: 4.605170185988092\n",
      "log loss for weighted knn: 5.756462732485114\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 8.059074478726833\n",
      "log loss for weighted knn: 8.059074478726833\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 8.059074478726833\n",
      "log loss for weighted knn: 8.059074478726833\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 8.059074478726833\n",
      "log loss for weighted knn: 8.059074478726833\n",
      "log loss for weighted knn: 8.059074478726833\n",
      "log loss for weighted knn: 8.059074478726833\n",
      "log loss for weighted knn: 6.907781932229811\n",
      "log loss for weighted knn: 8.059074478726833\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for weighted knn: 6.907808585477483\n",
      "log loss for weighted knn: 6.907808585477483\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "initial_weights = np.zeros(train_df.shape[1] - 2)\n",
    "result = minimize(log_loss_for_weighted_knn, initial_weights, args=(train_df, test_df, 5, 'label'), \n",
    "                  method='L-BFGS-B', bounds=[(0, 2)],\n",
    "                  options={'disp': True, 'eps': 1e-2})\n",
    "disp=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_weights = result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31578986, 2.        , 0.        , 0.        , 0.        ,\n",
       "       0.31578986, 0.63157972, 2.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.31578986, 0.31578986, 0.31578986])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Weighted and Vanilla KNN Classification:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss for weighted knn: 5.756489385732788\n",
      "normal knn result error : 5.756489385732788\n",
      "log loss for weighted knn: 5.756489385732788\n",
      "weighted knn result error : 5.756489385732788\n"
     ]
    }
   ],
   "source": [
    "normal_knn_error = log_loss_for_weighted_knn([1 for i in range(15)],train_df,test_df)\n",
    "print(f'normal knn result error : {normal_knn_error}')\n",
    "\n",
    "weighted_knn_error = log_loss_for_weighted_knn(optimized_weights,train_df,test_df)\n",
    "print(f'weighted knn result error : {weighted_knn_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Dimensionality Reduction Using Random Feature Subsets:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f8', 'f13', 'f15', 'f5', 'f13']\n",
      "Reselecting\n",
      "['f6', 'f4', 'f3', 'f13', 'f12']\n",
      "log loss for weighted knn: 12.66432462445794\n",
      "['f3', 'f10', 'f6', 'f9', 'f6']\n",
      "Reselecting\n",
      "['f10', 'f8', 'f9', 'f11', 'f7']\n",
      "log loss for weighted knn: 13.815643824202636\n",
      "['f3', 'f3', 'f2', 'f11', 'f1']\n",
      "Reselecting\n",
      "['f5', 'f11', 'f8', 'f15', 'f12']\n",
      "log loss for weighted knn: 10.361739531463895\n",
      "['f9', 'f12', 'f1', 'f10', 'f8']\n",
      "log loss for weighted knn: 20.723532369423136\n",
      "['f12', 'f10', 'f12', 'f5', 'f4']\n",
      "Reselecting\n",
      "['f13', 'f12', 'f10', 'f14', 'f11']\n",
      "log loss for weighted knn: 17.269548116941376\n",
      "['f3', 'f6', 'f2', 'f1', 'f1']\n",
      "Reselecting\n",
      "['f10', 'f14', 'f14', 'f10', 'f15']\n",
      "Reselecting\n",
      "['f9', 'f12', 'f2', 'f14', 'f5']\n",
      "log loss for weighted knn: 8.059074478726833\n",
      "['f5', 'f8', 'f15', 'f2', 'f13']\n",
      "log loss for weighted knn: 8.059101131974506\n",
      "['f5', 'f8', 'f5', 'f8', 'f7']\n",
      "Reselecting\n",
      "['f1', 'f5', 'f12', 'f3', 'f10']\n",
      "log loss for weighted knn: 11.513085384456266\n",
      "best error : 8.059074478726833 \n",
      "best features subset : ['f9', 'f12', 'f2', 'f14', 'f5', 'label']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import traceback\n",
    "\n",
    "def select_5_subset_features():\n",
    "    feature_subset = []\n",
    "    feature_indices = []\n",
    "    for i in range(5):\n",
    "        random_ = random.randint(1,15)\n",
    "        feature_subset.append(f'f{random_}')\n",
    "        feature_indices.append(random_)\n",
    "    return feature_subset,feature_indices\n",
    "\n",
    "best_feature_subsets = []\n",
    "best_feature_indices = []\n",
    "best_error = 20\n",
    "i = 0\n",
    "while i < 8:\n",
    "    try:\n",
    "        feature_s ,feature_i = select_5_subset_features()\n",
    "        print(feature_s)\n",
    "        feature_s.append('label')\n",
    "        tr_df = train_df[feature_s]\n",
    "        te_df = test_df[feature_s]\n",
    "        #  weights =  optimized_weights[feature_i]\n",
    "        weights = [1,1,1,1,1]\n",
    "        error = log_loss_for_weighted_knn(weights,tr_df,te_df)\n",
    "        if error <  best_error:\n",
    "            best_error = error\n",
    "            best_feature_subsets = feature_s\n",
    "            best_feature_indices = feature_i\n",
    "        i += 1\n",
    "    except Exception as e:\n",
    "        print(f'Reselecting')\n",
    "\n",
    "print(f'best error : {best_error} \\nbest features subset : {best_feature_subsets}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Feature Selection Based on Weights:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_of_top_5 = np.argsort(optimized_weights)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 14,  6,  1,  7])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_of_top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31578986, 0.31578986, 0.63157972, 2.        , 2.        ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_weights[indices_of_top_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f14', 'f15', 'f7', 'f2', 'f8']\n",
      "log loss for weighted knn: 9.210393678471528\n",
      "error of the selected 5 features : 9.210393678471528\n"
     ]
    }
   ],
   "source": [
    "selected_features = []\n",
    "for i  in range(len(indices_of_top_5)):\n",
    "    index = indices_of_top_5[i] + 1\n",
    "    selected_features.append(f'f{index}')\n",
    "print(selected_features)\n",
    "columns = selected_features\n",
    "columns.append('label')\n",
    "tr_df = train_df[columns]\n",
    "te_df = test_df[columns]\n",
    "error = log_loss_for_weighted_knn(optimized_weights[indices_of_top_5],tr_df,te_df)\n",
    "print(f'error of the selected 5 features : {error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Comparison and Analysis:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss for weighted knn: 5.756489385732788\n",
      "log loss for no feature selection : 5.756489385732788\n",
      "log loss for weighted knn: 8.059074478726833\n",
      "log loss for random feature selection : 8.059074478726833\n",
      "log loss for weighted knn: 9.210393678471528\n",
      "log loss for weighted feature selection : 9.210393678471528\n"
     ]
    }
   ],
   "source": [
    "# no feature selection\n",
    "print(f'log loss for no feature selection : {log_loss_for_weighted_knn(weights=[1 for i in range(15)],train_df=train_df,test_df=test_df)}')\n",
    "# for random feature selection \n",
    "tr_df = train_df[best_feature_subsets]\n",
    "te_df = test_df[best_feature_subsets]\n",
    "print(f'log loss for random feature selection : {log_loss_for_weighted_knn(weights=[1 for i in range(5)],train_df=tr_df,test_df=te_df)}')\n",
    "# for weighted feature selection \n",
    "tr_df = train_df[columns]\n",
    "te_df = test_df[columns]\n",
    "print(f'log loss for weighted feature selection : {log_loss_for_weighted_knn(weights=optimized_weights[indices_of_top_5],train_df=tr_df,test_df=te_df)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
